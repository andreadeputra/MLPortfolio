# -*- coding: utf-8 -*-
"""Kalbe Nutritionals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mqDm0NuCMjGiEGsE5CgSrGDYWCzmod1W

# Kalbe Nutritionals - Machine Learning Project

In this project, I'm given the role as a data scientist in Kalbe Nutritionals. I am provided with 4 files from sales database to use for this project. The tasks for this project are as follow:
1. **Product quantity prediction**: Inventory department requested for total sales prediction for all products to make sure they have enough stock available daily
2. **Customer Segmentation**: Marketing department requested for clustering model to help make customer segmentations for personalized promotion and sales treatment in the future
"""

import pandas as pd
import numpy as np
import seaborn as sns
import plotly.express as px

from matplotlib import pyplot as plt
from matplotlib import rcParams
from sklearn.cluster import KMeans
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.preprocessing import StandardScaler
from statsmodels.graphics.tsaplots import plot_pacf, plot_acf
from statsmodels.tools.sm_exceptions import ValueWarning, ConvergenceWarning
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller, kpss, InterpolationWarning

pd.options.display.max_columns = None
pd.options.display.float_format = '{:.4f}'.format

custom_rc = {'figure.figsize': (12,8),
             'axes.labelsize': 14,
             'figure.titlesize': 18}
rcParams.update(custom_rc)

import warnings
warnings.simplefilter(action='ignore',
                      category=(InterpolationWarning, FutureWarning, ValueWarning, ConvergenceWarning, UserWarning))

plt.style.use('ggplot')

"""## Dataset Collection & Preparation"""

df_cust = pd.read_csv('Case Study - Customer.csv', delimiter=';')
df_prod = pd.read_csv('Case Study - Product.csv', delimiter=';')
df_stor = pd.read_csv('Case Study - Store.csv', delimiter=';')
df_trnx = pd.read_csv('Case Study - Transaction.csv', delimiter=';')

# Function to check if there is duplicates in column
def check_duplicate(df, col):
    '''
    df: Dataframe to check for duplicated columns
    col: Column or list of columns to check for duplicates

    Print result whether duplicates are found or not
    '''
    try:
        has_dupe = df[col].duplicated().any()
        if has_dupe:
            dupes = df[col].duplicated().sum()
            print(f'There are {dupes} duplicates found in {col}')
        else:
            print(f'There is no duplicate found in {col}')

    except Exception as e:
        print('Something is wrong in the parameter input')

"""### Customer Data Validation"""

df_cust.info()

"""Columns' descriptions provided by data engineer:
- **CustomerID**    : Unique customer's id number
- **Age**           : Customer's age
- **Gender**        : Customer's gender (0: Female, 1: Male)
- **Marital Status**: Customer's marital status (Married, Single)
- **Income**        : Customer's monthly income in millions rupiah
"""

check_duplicate(df_cust, 'CustomerID')

df_cust.head(3)

# Drop missing values since it only consists of <1% of the customer data
df_cust.dropna(inplace=True)

# Clean up income's value and datatype
df_cust['Income'] = df_cust['Income'].apply(lambda x: x.replace(',', '.'))
df_cust['Income'] = df_cust['Income'].astype('float')

# Change Gender column name to Male for interpretability
df_cust.rename(columns={'Gender': "Male"}, inplace=True)

df_cust.info()

plt.suptitle('Value Distribution for Customer Data')
for i, col in enumerate(['Age', 'Income']):
    plt.subplot(2,2,i+1)
    sns.boxplot(x=df_cust[col])

for i, col in enumerate(['Male', 'Marital Status']):
    plt.subplot(2,2,i+3)
    sns.countplot(x=df_cust[col])
    plt.ylabel('')

"""While there seems to be some outliers in **Income**, those numbers are still plausible. Thus, in this case they will be treated as valid outliers.

### Product Data Validation
"""

df_prod.info()

"""Columns' descriptions provided by data engineer:
- **ProductID**  : Unique product's id number
- **ProductName**: Product's name
- **Price**      : Product's price in rupiah
"""

check_duplicate(df_prod, 'ProductID')

df_prod.head()

"""Product data is already clean

### Store Data Validation
"""

df_stor.info()

"""Columns' descriptions provided by data engineer:
- **StoreID**   : Unique store's id code
- **StoreName** : Store's name
- **GroupStore**: Store's group name
- **Type**      : Store's type (Modern Trade, General Trade)
- **Latitude**  : Store's latitude coordinate
- **Longitude** : Store's longitude coordinate
"""

check_duplicate(df_stor, 'StoreID')

df_stor.head(3)

# Clean up latitude and longitude value and data type
df_stor['Latitude'] = df_stor['Latitude'].apply(lambda x: x.replace(',', '.'))
df_stor['Latitude'] = df_stor['Latitude'].astype('float')

df_stor['Longitude'] = df_stor['Longitude'].apply(lambda x: x.replace(',', '.'))
df_stor['Longitude'] = df_stor['Longitude'].astype('float')

df_stor.info()

"""### Transaction Data Validation"""

df_trnx.info()

"""Columns' descriptions provided by data engineer:
- **TransactionID**: Unique transactions's id code
- **CustomerID**   : Customer's id number in transaction
- **Date**         : Transaction's date
- **ProductID**    : Product's id number in transaction
- **Price**        : Product's price
- **Qty**          : Total quantity of product in the transaction
- **TotalAmount**  : Total amount spent in transaction (**Qty** x **Price**)
- **StoreID**      : Store's id code in transaction
"""

df_trnx.describe().T

df_trnx.describe(include='object').T

check_duplicate(df_trnx, 'TransactionID')

df_trnx[df_trnx.TransactionID.duplicated(keep=False)].sort_values(by='TransactionID').head()

"""It appears that there are duplicated **TransactionID** despite being completely unique judging by other columns. Some workaround involves combining **TransactionID** with **CustomerID** to create unique id. However, for the scope of this project, that might not be necessary so I will leave them as is."""

df_trnx['Date'] = pd.to_datetime(df_trnx['Date'], format='%d/%m/%Y')
df_trnx.head()

plt.suptitle('Value Distribution for Transaction Data')

# Quantity
plt.subplot(2,2,1)
sns.boxplot(x=df_trnx['Qty'])
plt.xlabel('Quantity Sold per Transaction')

# Revenue (price x qty)
plt.subplot(2,2,2)
g = sns.boxplot(x=df_trnx['TotalAmount'])
plt.xlabel('Total Revenue per Transaction')
xticks = [f'{x:.0f}K' for x in g.get_xticks()/1000]
plt.xticks(g.get_xticks()[1:-1], xticks[1:-1])
plt.show()

"""## Data Merge"""

# Inner join to remove missing values dropped from previous steps
df = pd.merge(df_trnx, df_cust,
              how='inner', on='CustomerID')
df = pd.merge(df, df_prod.drop('Price', axis=1),
              how='inner', on='ProductID')
df = pd.merge(df, df_stor,
              how='inner', on='StoreID')

# Rearrange columns
df = df[['TransactionID', 'CustomerID', 'Date',
        'Age', 'Male', 'Marital Status', 'Income',
        'ProductID', 'Product Name', 'Price', 'Qty', 'TotalAmount',
        'StoreID', 'StoreName', 'GroupStore', 'Type', 'Latitude', 'Longitude']]
df.head()

df.info()

"""## Time Series Regression for Product Quantity Prediction

As specified by inventory department's requirement, I am tasked to predict the product quantity available in stock based on available data. Before machine learning prediction can be done, it is necessary to preprocess the data to fit into a time series regression.
"""

# Daily sales data for year 2022
df_qty = df.groupby('Date').agg({'Qty': 'sum'})
df_qty.head()

df_qty.plot()
plt.title('Time Series Distribution for Daily Quantity Sold')
plt.legend('')
plt.show()

"""Based on the graph above, time series data for daily quantity sold seems to be stationary.

However, to make sure of data staionarity, some ADFuller and KPSS test will be done for the data.

### Stationarity Check
"""

def adf_stationarity(data, critical_value=0.05):
    adf = adfuller(data)

    if adf[1] < critical_value:
        print('ADFuller result: stationary')
    else:
        print('ADFuller result: not stationary')

def kpss_stationarity(data, critical_value=0.05):
    kpss_ = kpss(data)

    if kpss_[1] < critical_value:
        print('KPSS     result: not stationary')
    else:
        print('KPSS     result: stationary')

adf_stationarity(df_qty)
kpss_stationarity(df_qty)

"""Based on the result above, this data is [difference stationary](https://www.statsmodels.org/dev/examples/notebooks/generated/stationarity_detrending_adf_kpss.html#:~:text=Case%204%3A%20KPSS%20indicates%20non%2Dstationarity%20and%20ADF%20indicates%20stationarity%20%2D%20The%20series%20is%20difference%20stationary.%20Differencing%20is%20to%20be%20used%20to%20make%20series%20stationary.%20The%20differenced%20series%20is%20checked%20for%20stationarity.)."""

adf_stationarity(df_qty.diff().dropna())
kpss_stationarity(df_qty.diff().dropna())

"""Time series is stationary on both ADFuller and KPSS test on first difference.

### PACF and ACF Analysis

PACF and ACF will be used to estimate the optimal term for *p* and *q* for ARIMA model. On top of that, they will serve as an additional measure to test for optimal order of difference (*d* term)
"""

fig, axes = plt.subplots(2,3,)
month_start = df_qty.index[df_qty.index.is_month_start]

axes[0, 0].plot(df_qty.Qty)
axes[0, 0].set_title('Original Series')
axes[0, 0].set_xticks(month_start, [date.strftime('%b') for date in month_start], rotation=45)
plot_pacf(df_qty.Qty, ax=axes[0, 1], lags=40)
plot_acf(df_qty.Qty, ax=axes[0, 2], lags=40)

axes[1, 0].plot(df_qty.Qty.diff())
axes[1, 0].set_title('1st Order Difference')
axes[1, 0].set_xticks(month_start, [date.strftime('%b') for date in month_start], rotation=45)
plot_pacf(df_qty.Qty.diff().dropna(), ax=axes[1, 1], lags=40)
plot_acf(df_qty.Qty.diff().dropna(), ax=axes[1, 2], lags=40)

plt.tight_layout()

"""While the first order of differencing provides a more stationary time series, it's PACF plot shows an indication of [over differencing](https://www.linkedin.com/advice/1/how-do-you-deal-over-differencing-under-differencing#:~:text=If%20the%20ACF%20and%20PACF%20show%20significant%20spikes%20at%20high%20lags%2C%20or%20if%20the%20ACF%20decays%20very%20slowly%2C%20you%20may%20have%20over%2Ddifferenced%20your%20data.) that can be seen from a spike in PACF around 40 lags. Thus, I will be using 0 as *d* term for the ARIMA model.

For *p* and *q* term, they will be manually determined by looking for lags on PACF and ACF with y approaching 0 in the original time series. Based on that criteria, there are several numbers of lag such as:
- **p**: 2, 7, 12, 15, 24, 28, 31
- **q**: 2, 7, 13, 15, 22, 29, 31

For the scope of this project, I will choose 3 or 4 max from each to save time.

### Train Test Split
"""

# train test split 80:20
cutoff = round(df_qty.shape[0] * 0.8)
qty_train = df_qty[:cutoff]
qty_test = df_qty[cutoff:]

"""### ARIMA Batch Modeling"""

fig, axes = plt.subplots(4, 4)

p_terms = [2,7,15,31]
q_terms = [2,7,15,29]

eval_table = {'model': [],
              'mae': [],
              'mape': [],
              'rmse': []}

for i, p  in enumerate(p_terms):
    for j, q in enumerate(q_terms):
        # Run model
        model = ARIMA(qty_train, order=(p,0,q))
        model_fit = model.fit()
        model_name = f'ARIMA ({p}, 0, {q})'
        fc = model_fit.get_forecast(len(qty_test))
        pred = fc.predicted_mean

        # Evaluate model results
        mae = round(mean_absolute_error(qty_test, pred), 4)
        mape = round(mean_absolute_percentage_error(qty_test, pred), 4)
        rmse = round(mean_squared_error(qty_test, pred, squared=False), 4)
        eval_table['model'].append(model_name)
        eval_table['mae'].append(mae)
        eval_table['mape'].append(mape)
        eval_table['rmse'].append(rmse)

        # Plot forecast
        month_start = df_qty[200:].index[df_qty[200:].index.is_month_start]
        axes[i, j].plot(fc.predicted_mean, label='Forecast')
        axes[i, j].plot(df_qty[200:], label='Actual', alpha=.25, c='b')
        axes[i, j].fill_between(fc.conf_int().index,
                        fc.conf_int()['lower Qty'],
                        fc.conf_int()['upper Qty'],
                        color='k', alpha=.1)
        axes[i, j].set_xticks(month_start,
                              [date.strftime('%b') for date in month_start],
                              rotation=45)
        axes[i, j].set_title(model_name)

plt.tight_layout()
plt.legend(bbox_to_anchor=(1,1), fontsize=14, loc='upper left')
plt.show()

"""Based on the graph above, the graphs can be classified into:
1. General flatlining: where the model only manage to generalize guess without capturing pattern, examples are (2,0,7) and (7,0,2)
2. Fluctuating flatline: where the model learned enough pattern fluctuations, but failed to capture fluctuation in trends, examples are (7,0,7), (7,0,15), and (15,0,7)
3. General trend: where the model managed to fit into the general trend only, the only example is (31,0,2)
4. Partial good fit: where the model either managed to fit well at certain range into the predictions, examples are (2,0,29), (7,0,29), (15,0,29), (31,0,7), (31,0,15), and (31,0,29)

Unfortunately, there is no model that fits well enough to predict the drop near the beginning of December. However, for this project, I will take a closer look on the model with partial good fit.
"""

# Top 10 ARIMA model based on MAPE
eval_df = pd.DataFrame(eval_table)
eval_df.sort_values(by='mae').head(10)

"""Based on evaluation table above, all models have very close error to each other despite having very much different visualization. Thus, error metrics will be used as additional criteria for model selection for this case.

### ARIMA with Partial Good Fit
"""

fig, axes = plt.subplots(3,2)
selected_orders = [( 2, 0, 29), ( 7, 0, 29),
                   (15, 0, 29), (31, 0,  7),
                   (31, 0, 15), (31, 0, 29)]

eval_table = {'model': [],
              'mae': [],
              'mape': [],
              'rmse': []}

for i, order  in enumerate(selected_orders):
    # Run model
    model = ARIMA(qty_train, order=order)
    model_fit = model.fit()
    model_name = f'ARIMA {order}'
    fc = model_fit.get_forecast(len(qty_test))
    pred = fc.predicted_mean

    # Evaluate model results
    mae = round(mean_absolute_error(qty_test, pred), 4)
    mape = round(mean_absolute_percentage_error(qty_test, pred), 4)
    rmse = round(mean_squared_error(qty_test, pred, squared=False), 4)
    eval_table['model'].append(model_name)
    eval_table['mae'].append(mae)
    eval_table['mape'].append(mape)
    eval_table['rmse'].append(rmse)

    # Plot forecast
    month_start = df_qty[cutoff-30:].index[df_qty[cutoff-30:].index.is_month_start]
    ax = axes[i//2, i%2]
    ax.plot(fc.predicted_mean, label='Forecast')
    ax.plot(df_qty[cutoff-30:], label='Actual', alpha=.25, c='b')
    ax.fill_between(fc.conf_int().index,
                    fc.conf_int()['lower Qty'],
                    fc.conf_int()['upper Qty'],
                    color='k', alpha=.1)
    ax.set_xticks(month_start,
                            [date.strftime('%b') for date in month_start],
                            rotation=45)
    ax.set_title(model_name)

plt.tight_layout()
plt.legend(bbox_to_anchor=(-0.3,-0.2), fontsize=14, loc='upper left')
plt.show()

"""In general, most of them can grasp the first 2 weeks to 1 month prediction quite well. Based on the criteria, notable good fit for the first month prediction are:
1. (7,0,29)
2. (15,0,29): notably the only model that can predict the trough near the beginning of prediction
3. (31,0,29)

On the other hand, if the criteria is the whole test data or over 2 months prediction (31,0,29) might be the best overall.

### Selected Model: ARIMA (31,0,29)
"""

arima = ARIMA(df_qty, order=(31,0,29))
model_fit = arima.fit()
# model_fit.summary()

fc = model_fit.get_forecast(steps=len(qty_test))

plt.plot(fc.predicted_mean, label='Forecast')
plt.plot(df_qty, label='Actual', alpha=.25, c='b')
plt.fill_between(fc.conf_int().index,
                 fc.conf_int()['lower Qty'],
                 fc.conf_int()['upper Qty'],
                 color='k', alpha=.1)

plt.title(f'Forecast into {len(qty_test)} Days')
plt.legend(loc=1)
plt.show()

"""As expected from previous evaluation, the prediction starts of promising but starts to converge for further prediction into the future. For that reason, this model might be used for predicting shorter duration like 1 month or less."""

fc = model_fit.get_forecast(steps=30)

plt.plot(fc.predicted_mean, label='Forecast')
plt.plot(df_qty, label='Actual', alpha=.25, c='b')
plt.fill_between(fc.conf_int().index,
                 fc.conf_int()['lower Qty'],
                 fc.conf_int()['upper Qty'],
                 color='k', alpha=.1)

plt.title('Forecast into 30 Days')
plt.legend(loc=1)
plt.show()

avg_qty = np.mean(fc.predicted_mean)
print(f'Average daily quantity sold for the next 30 days: {avg_qty:.0f} pcs')

"""Based on that result, Kalbe should prepare inventory of around 1440 products in the next 30 days. Since the scope of this project doesn't include prediction for different product, it's difficult to recommend on how many stocks to prepare for each products.

That being said, the recommendations to be considered are as follow:
- Prepare 144-150 stocks of each products and refresh said number of stocks every months
- Do further analysis for quantity sold rate of each products

## Customer Segmentation

As specified by marketing department's requirement, I am tasked to make customer segmentation for personalized promotion purposes. The minimum requirements given by marketing department are transaction frequency, total quantity purchased, and total amount spent. To complete RFM analysis component of 'recency', I will also include last purchase date of each customers.
"""

# Customer's transaction
df_clust = df.groupby('CustomerID').agg({'TransactionID': 'count',
                                         'Qty': 'sum',
                                         'TotalAmount': 'sum',
                                         'Date': 'max'}).reset_index()
df_clust.columns = ['CustomerID', 'TotalTranx', 'TotalPurchased', 'TotalSpending', 'LastPurchaseDate']
df_clust['LastPurchase'] = pd.to_datetime('2023-01-01') - df_clust['LastPurchaseDate']
df_clust['LastPurchase'] = df_clust['LastPurchase'].apply(lambda x: x.days)
df_clust.drop('LastPurchaseDate', axis=1, inplace=True)
df_clust.set_index('CustomerID', inplace=True)
df_clust.head()

"""### Feature Scaling"""

feats = df_clust.columns
plt.suptitle('Feature Distribution')
for i, feat in enumerate(feats):
    plt.subplot(2,2,i+1)
    g = sns.kdeplot(df_clust[feat])
    if feat == 'TotalSpending':
        xticks = [f'{x:.0f}K' for x in g.get_xticks()/1000]
        plt.xticks(g.get_xticks()[1:-1], xticks[1:-1])

"""There seems to be some outliers in **LastPurchase** columns. However since, it's important customer data for clustering, outliers will be included. Next will be to scale the features with [standard scaler](https://www.linkedin.com/advice/3/what-some-best-practices-preprocessing-scaling#:~:text=k%2Dmeans%20assumes%20that%20the%20clusters%20are%20spherical%20and%20have%20similar%20sizes%2C%20so%20it%20might%20benefit%20from%20standard%20scaling) to prepare for K-Means clustering."""

scaler = StandardScaler()
scaler.fit(df_clust)

x = scaler.transform(df_clust)
df_unlabel = pd.DataFrame(x, columns=feats, index=df_clust.index)
df_unlabel.head()

"""### Determining Optimal Cluster"""

inertia = []

for i in range(1,7):
    kmeans = KMeans(n_clusters=i, random_state=0)
    kmeans.fit(df_unlabel)
    inertia.append(kmeans.inertia_)

plt.xticks(list(range(1,7)))
plt.plot(range(1,7),inertia,marker='o')
plt.title('Elbow Method')
plt.show()

eval_cluster = pd.DataFrame({'n_clusters': list(range(1,7)),
                             'inertia': inertia,
                             'diff': pd.Series(inertia).diff(),
                             '% reduction vs before': pd.Series(inertia).diff() * -100 / inertia,
                             })
eval_cluster

"""Based on inertia and elbow method above, the optimal amount of cluster is at n_clusters between 3 or 4 where further reduction is not as significant."""

silh_score = {'n_cluster': [],
              'silhouette score': []}

df_clust_pred = df_clust.copy()

for n in range(3,6):
    fig, axes = plt.subplots(1,2, figsize=(12,6))
    colors = plt.cm.Spectral(np.linspace(0, 1, n))

    # Run model
    model = KMeans(n_clusters=n, random_state=0)
    labels = model.fit_predict(df_unlabel)

    # Silhouette score: average and score per samples
    silhouette_avg = silhouette_score(df_unlabel, labels)
    silhouette_values = silhouette_samples(df_unlabel, labels)

    # Silhouette plot
    axes[0].set_xlim([-0.2, 1])
    axes[0].set_ylim([0, df_unlabel.shape[0] ])
    offset = 1
    y_lower = 1
    for i in range(n):
        sample_silh_values = silhouette_values[labels == i]
        sample_silh_values.sort()

        cluster_size = sample_silh_values.shape[0]
        y_upper = y_lower + cluster_size

        axes[0].fill_betweenx(np.arange(y_lower, y_upper),
                              0, sample_silh_values,
                              facecolor=colors[i],
                              edgecolor='black',
                              alpha=0.7)
        axes[0].text(-0.05, y_lower + 0.5 * cluster_size, str(i))

        y_lower = y_upper + offset

    axes[0].axvline(x=silhouette_avg, color="red", linestyle="--")

    # Plot cluster prediction
    df_clust_pred['label'] = labels
    axes[1] = sns.scatterplot(data=df_clust_pred,
                              x='LastPurchase', # Recency
                              size='TotalTranx', # Frequency
                              y='TotalSpending', # Monetary
                              hue='label')

    plt.suptitle((f"KMeans clustering with {n} n_clusters\nSilhouette Score: {silhouette_avg:.3f}"),
                 fontsize=14, fontweight='bold')

plt.show()

"""Unfortunately, none of this model pass the 0.5 silhouette score threshold for high-quality cluster. However, this is good enough considering the data has several overlapping samples.

N clusters for optimal model is determined using the following steps:
1. N_clusters 5 is eliminated for having the lowest silhouette score
2. Comparing silhouette score between n_clusters 3 and 4, n_clusters 4 has slight edge with 0.332 score
3. N_clusters 3 is eliminated for having more negative sample scores, indicating wrong cluster classification
4. **N_clusters 4** is taken as the most optimal model

### K-Means Cluster Modeling
"""

# Run optimal cluster model
kmeans = KMeans(n_clusters=4,
                init='k-means++',max_iter=300,n_init=10,random_state=0)
cluster_fit = kmeans.fit(df_unlabel)

# Put cluster label on pre-scaled df
df_label = df_clust.copy()
df_label['label'] = cluster_fit.labels_
df_label.reset_index(inplace=True)
df_label.head()

sns.pairplot(df_label.drop('CustomerID', axis=1), hue='label')
plt.suptitle('Pairplot for Cluster Visualization', y=1.02)
plt.show()

"""4 clusters can be clearly seen on pairplot with **LastPurchase** as one of the features.

For better visualization, 3d graph might give better view for the clusters.
"""

fig = px.scatter_3d(df_label,
                    x='LastPurchase', # Recency
                    y='TotalTranx', # Frequency
                    z='TotalSpending', # Monetary
                    color='label')
fig.show()

"""### Customer Segmentation based on [RFM](https://documentation.bloomreach.com/engagement/docs/rfm-segmentation#:~:text=than%20five%20minutes.-,RFM%20segments,-Based%20on%20the)

For simplifications, each criteria of RFM will be divided into 'Low', 'Medium', and 'High' as specified below:
- Recency is considered 'High', the **lower** LastPurchase is
- Frequency is considered 'High', the **higher** TotalTranx is
- Monetary is considered 'High', the **higher** TotalSpend is

1. Label 0 (**Potential Loyalists**):
    - Recency : High
    - Frequency : Medium
    - Monetary : Medium
    - Recommendation : **Offer personalized recommendations** to increase their range of product purchases. The more product variety they are engaged with, the more likely for them to have more purchases in the future.

2. Label 1 (**New Customers**):
    - Recency : High
    - Frequency : Low
    - Monetary : Low
    - Recommendation : **Offer get-one-free promotion** on their purchases. The additional free product can be the same or different product. The goal is to push more purchase while increasing the engagement towards all products.

3. Label 2 (**Champions**):
    - Recency : High
    - Frequency : High
    - Monetary : High
    - Recommendation : **Offer loyalty-program discounts** to encourage more bulk purchases from them. They are already loyal, so this will reduce the possibility of opportunistic buyers who's aiming for only discount purchase.

4. Label 3 (**Hibernating Customers**):
    - Recency : Low
    - Frequency : Low
    - Monetary : Low
    - Recommendation : **Offer high discounted promotions** for new products, whether new addition or product they never engaged with before. A possible cause customers might go hibernation is that they weren't satisfied with what they purchased before, so a different product might just be more interesting than their previous purchases.
"""